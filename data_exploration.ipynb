{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python386jvsc74a57bd0d1bf8b0d71e2aa9c2cad3c0c4e42b040e81b49fe030ef6be7f4c063ee3654c78",
   "display_name": "Python 3.8.6 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "d1bf8b0d71e2aa9c2cad3c0c4e42b040e81b49fe030ef6be7f4c063ee3654c78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Neste notebook iremos efetuar uma pequena exploração de dados. Iremos também calcular a frequência das palavras em todos as publicações"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import scispacy\n",
    "import spacy\n",
    "import en_core_sci_lg\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "source": [
    "De forma a reduzirmos o número de palavras utilizamos funcionalidades do nltk para criar a função cleaning() que nos permite verificar se uma palavra é ou não stop word.\n",
    "Para além disso convertemos todas as palavras para minuscula, removemos link e carateres especiais."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_sci_lg.load(disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "nlp.max_length = 3000000\n",
    "\n",
    "def cleaning(text):\n",
    "    \"\"\"\n",
    "    Convert to lowercase.\n",
    "    Rremove URL links, special characters and punctuation.\n",
    "    Tokenize and remove stop words.\n",
    "    \"\"\"\n",
    "    if(isinstance(text, str)):\n",
    "        text = str(text.lower())\n",
    "        text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "        text = re.sub('<.*?>+', '', text)\n",
    "        text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "        text = re.sub('\\n', '', text)\n",
    "        text = re.sub('[’“”…]', '', text)\n",
    "\n",
    "        # removing the stop-words\n",
    "        text_tokens = word_tokenize(text)\n",
    "        tokens_without_sw = [word.lemma_ for word in nlp(text) if not (word.like_num or word.is_stop or word.is_punct or word.is_space or len(word)==1)]\n",
    "        filtered_sentence = (\" \").join(tokens_without_sw)\n",
    "        text = filtered_sentence\n",
    "\n",
    "        return text"
   ]
  },
  {
   "source": [
    "De seguida vamos ler o dataset das publicações e aplicar-lhe a função de cleaning()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv to pandas dataframe\n",
    "df = pd.read_csv('dataset_gastric_cancer.csv', sep='#')\n",
    "#apply cleaning() to dataframe\n",
    "dt = df['summary'].apply(cleaning)\n",
    "\n",
    "#calculate words frequency\n",
    "word_count = Counter(\" \".join(filter(None, dt)).split()).most_common(100000000000)\n",
    "\n",
    "#create dataframe with words frequency\n",
    "word_count = pd.DataFrame(word_count, columns = ['Word', 'Frequency'])\n",
    "\n",
    "#save words frequency dataframe\n",
    "word_count.to_csv('word_count1.csv',index=False)"
   ]
  },
  {
   "source": [
    "Apresentar as 20 palavras mais frequentes e menos frequentes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency.sort_values('Frequency', ascending=False).set_index('Word')[:20].sort_values('Frequency', ascending=True).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency.sort_values('Frequency', ascending=True).set_index('Word')[:20].sort_values('Frequency', ascending=True).plot(kind='barh')"
   ]
  }
 ]
}