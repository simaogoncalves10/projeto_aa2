{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python386jvsc74a57bd0d1bf8b0d71e2aa9c2cad3c0c4e42b040e81b49fe030ef6be7f4c063ee3654c78",
   "display_name": "Python 3.8.6 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "d1bf8b0d71e2aa9c2cad3c0c4e42b040e81b49fe030ef6be7f4c063ee3654c78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    " ## **Criação e treino do LDA**\n",
    "\n",
    "O objetivo do LDA para este problema é construir um modelo que permita distinguir os documentos entre as várias sub-doenças através da similaridade de cada documento a cada tópico de uma subdoença. Ou seja, os documentos relativos a uma sub-doença devem ser associados apenas a um tópico."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import scispacy\n",
    "import spacy\n",
    "import en_core_sci_lg\n",
    "import joblib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "from ipywidgets import interact, Layout, HBox, VBox, Box\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "source": [
    "Em primeiro lugar vamos carregar os dados que vamos utilizar para a criação dos modelos.\n",
    "Vamos ler o dataset_gastric_cancer que contém o dataset completo para um pandas dataframe e o dataset_reduced que contém as primeiras 100 palavras de cada publicação.\n",
    "\n",
    "Também iremos ler o pubs_by_disease que contém 50 publicações de cada uma das 32 subdoenças para depois podermos usá-las para validar o modelo\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/clean.csv', sep='#')\n",
    "\n",
    "pubs = pd.read_csv('data/pubs_by_disease.csv', sep='#')\n",
    "doids=pubs[\"doid\"].unique()"
   ]
  },
  {
   "source": [
    "De seguida carregamos um modelo de nlp treinado para documentos de biomedicina. Assim poderemos remover as palavras com um carater, os espaços e remover as palavas com sinais de pontuação"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medium model\n",
    "nlp=en_core_sci_lg.load(disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "nlp.max_length = 3000000\n",
    "\n",
    "def spacy_tokenizer(sentence):\n",
    "    return [word for word in nlp(sentence) if not (word.is_punct or word.is_space or len(word)==1)]"
   ]
  },
  {
   "source": [
    "Como as palavras que aparecem com grande ou pouco frequência no dataset podem ter influencia no desempenho do modelo, criamos a função create_customize_words() que irá definir se adicionamos as palavras muito frequentes e pouco frequentes às stop_words."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_customize_words():\n",
    "    #read words and frequency\n",
    "    words=pd.read_csv('data/word_count.csv')\n",
    "\n",
    "    customize_stop_words=[]\n",
    "    for index, row in words.iterrows():\n",
    "        if(row['Frequency']>1): customize_stop_words.append(row['Word'])\n",
    "\n",
    "    return customize_stop_words"
   ]
  },
  {
   "source": [
    "Agora, de forma a criarmos e treinarmos o modelo, implementamos a função train_lda() que nos permite definir o treino e as suas carateristicas.\n",
    "Tais como o dataset a usar e se vamos eliminar ou não as palavras mais frequentes\n",
    "\n",
    "Fora estas pequenas alterações, o que a função faz em geral é:\n",
    "\n",
    "1-Definir as stop_words\n",
    "\n",
    "2-Vetorizar o texto\n",
    "\n",
    "3-Inicializar o LDA com 32 componentes\n",
    "\n",
    "4-Treinar o LDA com o texto vetorizado\n",
    "\n",
    "5-Calcular as distancias de cada elemento do dataset a cada tópico\n",
    "\n",
    "6-Executar a função print_top_words()\n",
    "\n",
    "7-Guardas os objetos resultados para poderem ser usados mais tarde"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lda(df,folder_path,delete_by_frequency):\n",
    "    if(not os.path.isdir(folder_path)):\n",
    "        os.mkdir(folder_path)\n",
    "\n",
    "    customize_stop_words=[]\n",
    "    #New stop words list \n",
    "    if(delete_by_frequency): customize_stop_words = create_customize_words()\n",
    "\n",
    "    # Mark them as stop words\n",
    "    for w in customize_stop_words:\n",
    "        if(not isinstance(w, float)): nlp.vocab[w].is_stop = True\n",
    "\n",
    "    #Convert a collection of text documents to a matrix of token counts\n",
    "    vectorizer = CountVectorizer(tokenizer = spacy_tokenizer, min_df=2)\n",
    "\n",
    "    #Learn the vocabulary dictionary and return document-term matrix.\n",
    "    #The astype(‘U’) is telling numpy to convert the data to Unicode (essentially a string in python 3)\n",
    "    data_vectorized = vectorizer.fit_transform(df['summary'].values.astype('U'))\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=32, random_state=0)\n",
    "    lda.fit(data_vectorized)\n",
    "\n",
    "    #get topic distances\n",
    "    doc_topic_dist = pd.DataFrame(lda.transform(data_vectorized))\n",
    "\n",
    "    print_top_words(lda, vectorizer, n_top_words=25)\n",
    "\n",
    "    #joblib.dump Persist an arbitrary Python object into one file.\n",
    "    joblib.dump(vectorizer, folder_path+'/vectorizer.csv')\n",
    "    joblib.dump(data_vectorized,folder_path+'/data_vectorized.csv')\n",
    "    joblib.dump(lda, folder_path+'/lda.csv')\n",
    "    doc_topic_dist.to_csv(folder_path+'/doc_topic_dist.csv', index=False)"
   ]
  },
  {
   "source": [
    "Também definimos uma função que irá permitir perceber o funcionamento do LDA treinado. A função print_top_words() vai apresentar na consola os tópicos textuais mais importantes para cada um dos componentes, ou seja, sub-doenças"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, vectorizer, n_top_words):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"\\nTopic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)"
   ]
  },
  {
   "source": [
    "De forma analisarmos as previsões do LDA para um conjunto de publicaçoes, definimos a função dna_tabs que permite para cada documento representar a similaridade do mesmo para cada um dos tópicos"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_article_dna(paper_id, width=20):\n",
    "    doc_topic_dist[df[\"id\"] == paper_id].T.plot(kind='bar', legend=None, figsize=(width, 4))\n",
    "    plt.xlabel('Topic')\n",
    "\n",
    "def dna_tabs(paper_ids):\n",
    "    k = len(paper_ids)\n",
    "    outs = [widgets.Output() for i in range(k)]\n",
    "\n",
    "    tab = widgets.Tab(children = outs)\n",
    "    tab_titles = ['Paper ' + str(i+1) for i in range(k)]\n",
    "    for i, t in enumerate(tab_titles):\n",
    "        tab.set_title(i, t)\n",
    "    display(tab)\n",
    "\n",
    "    for i, t in enumerate(tab_titles):\n",
    "        with outs[i]:\n",
    "            ax = plot_article_dna(paper_ids[i])\n",
    "            plt.show(ax)"
   ]
  },
  {
   "source": [
    "Para que consigamos validar o desempenho do LDA usamos o dataset pubs_by_disease. Com isto verificamos se a previsão dos documentos da mesma sub-doença pertenciam a apenas um tópico e se esse tópico não se repete para mais nenhuma subdoença"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty(doid, frequencia, indent=0):\n",
    "   d=dict(enumerate(frequencia.flatten(), 0))\n",
    "   print(\"Subdoença \" + str(doid) )\n",
    "   for key, value in d.items():\n",
    "      if(value!=0): print('Tópico ' + str(key) + \" ------ \" +  str(value))\n",
    "   print()\n",
    "\n",
    "def lda_validation(doids, pubs, doc_topic_dist):\n",
    "    for doid in doids:\n",
    "        frequencia=np.zeros(100)\n",
    "        p=pubs[pubs[\"doid\"]==doid].id\n",
    "        for id in p:\n",
    "            try:\n",
    "                distancias=doc_topic_dist[df[\"id\"] == id]\n",
    "                max_indice=np.argmax(distancias)\n",
    "                frequencia[max_indice]+=1\n",
    "            except: \n",
    "                pass\n",
    "        pretty(doid, frequencia)"
   ]
  },
  {
   "source": [
    "## Modelo 1 \n",
    "\n",
    "Para este modelo usamos os dataset clean.csv de modo a obtermos um modelo de base"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lda(df,'modelo1',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\jpmrs\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:310: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.24.2 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jpmrs\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:310: UserWarning: Trying to unpickle estimator LatentDirichletAllocation from version 0.24.2 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vectorizer = joblib.load('modelo1/vectorizer.csv')\n",
    "data_vectorized = joblib.load('modelo1/data_vectorized.csv')\n",
    "lda = joblib.load('modelo1/lda.csv') \n",
    "doc_topic_dist = pd.read_csv('modelo1/doc_topic_dist.csv')  "
   ]
  },
  {
   "source": [
    "De seguida, deixo exemplos da distribuição das publicações das subdoenças 0, 1 e 25 pelos diveros tópicos"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Tab(children=(Output(), Output(), Output(), Output(), Output(), Output(), Output(), Output(), Output(), Output…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f7457daaf1b6455aa998d353e6fabcbd"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "#documentos da sub doença 0\n",
    "papers=pubs[pubs[\"doid\"]==doids[0]]\n",
    "\n",
    "lista=[]\n",
    "[lista.append(papers.iloc[i].id) for i in range(1,len(papers))]\n",
    "\n",
    "dna_tabs(lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#documentos da sub doença 0\n",
    "papers=pubs[pubs[\"doid\"]==doids[1]]\n",
    "\n",
    "lista=[]\n",
    "[lista.append(papers.iloc[i].id) for i in range(1,len(papers))]\n",
    "\n",
    "dna_tabs(lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Tab(children=(Output(), Output(), Output(), Output(), Output(), Output(), Output(), Output(), Output(), Output…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "906f331819e246e8a625631702158bc8"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "#documentos da sub doença 25\n",
    "papers=pubs[pubs[\"doid\"]==doids[25]]\n",
    "\n",
    "lista=[]\n",
    "[lista.append(papers.iloc[i].id) for i in range(1,len(papers))]\n",
    "\n",
    "dna_tabs(lista)"
   ]
  },
  {
   "source": [
    "Analisando os gráficos para estas três subdoenças podemos perceber que nem todas as publicações de uma mesma sub-doença estão associadas apenas a um tópico. No entanto podemos notar que existe uma tendencia como é o caso do tópico 15 para a doença 0, da doença 1 para o tópico 15 e da doença 25 para o tópico 3\n",
    "\n",
    "Porém, analisando melhor a distribuição da associação das publicaçoes às sub-doenças podemos reparar através dos resultados da função seguinte que maioria dos documentos de todas as subdoenças estão a ser associados ao tópico 14."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Subdoença 4716\n",
      "Tópico 1 ------ 6.0\n",
      "Tópico 3 ------ 2.0\n",
      "Tópico 4 ------ 1.0\n",
      "Tópico 7 ------ 2.0\n",
      "Tópico 11 ------ 3.0\n",
      "Tópico 14 ------ 15.0\n",
      "Tópico 16 ------ 3.0\n",
      "Tópico 17 ------ 3.0\n",
      "Tópico 18 ------ 1.0\n",
      "Tópico 25 ------ 3.0\n",
      "Tópico 26 ------ 2.0\n",
      "Tópico 27 ------ 6.0\n",
      "Tópico 29 ------ 2.0\n",
      "Tópico 30 ------ 1.0\n",
      "\n",
      "Subdoença 8025\n",
      "Tópico 1 ------ 1.0\n",
      "Tópico 2 ------ 1.0\n",
      "Tópico 7 ------ 3.0\n",
      "Tópico 11 ------ 1.0\n",
      "Tópico 14 ------ 35.0\n",
      "Tópico 15 ------ 2.0\n",
      "Tópico 21 ------ 1.0\n",
      "Tópico 25 ------ 3.0\n",
      "Tópico 27 ------ 2.0\n",
      "Tópico 29 ------ 1.0\n",
      "\n",
      "Subdoença 10538\n",
      "Tópico 0 ------ 1.0\n",
      "Tópico 2 ------ 1.0\n",
      "Tópico 3 ------ 7.0\n",
      "Tópico 5 ------ 3.0\n",
      "Tópico 7 ------ 3.0\n",
      "Tópico 9 ------ 5.0\n",
      "Tópico 10 ------ 2.0\n",
      "Tópico 12 ------ 1.0\n",
      "Tópico 14 ------ 14.0\n",
      "Tópico 15 ------ 4.0\n",
      "Tópico 20 ------ 1.0\n",
      "Tópico 21 ------ 1.0\n",
      "Tópico 24 ------ 1.0\n",
      "Tópico 25 ------ 4.0\n",
      "Tópico 27 ------ 1.0\n",
      "\n",
      "Subdoença 10540\n",
      "Tópico 2 ------ 1.0\n",
      "Tópico 3 ------ 4.0\n",
      "Tópico 4 ------ 2.0\n",
      "Tópico 7 ------ 22.0\n",
      "Tópico 10 ------ 1.0\n",
      "Tópico 14 ------ 11.0\n",
      "Tópico 15 ------ 5.0\n",
      "Tópico 25 ------ 3.0\n",
      "Tópico 28 ------ 1.0\n",
      "\n",
      "Subdoença 5593\n",
      "Tópico 2 ------ 6.0\n",
      "Tópico 3 ------ 1.0\n",
      "Tópico 4 ------ 1.0\n",
      "Tópico 7 ------ 13.0\n",
      "Tópico 14 ------ 16.0\n",
      "Tópico 15 ------ 3.0\n",
      "Tópico 18 ------ 1.0\n",
      "Tópico 25 ------ 1.0\n",
      "Tópico 27 ------ 7.0\n",
      "Tópico 29 ------ 1.0\n",
      "\n",
      "Subdoença 6217\n",
      "Tópico 0 ------ 1.0\n",
      "Tópico 1 ------ 1.0\n",
      "Tópico 3 ------ 1.0\n",
      "Tópico 4 ------ 1.0\n",
      "Tópico 5 ------ 3.0\n",
      "Tópico 7 ------ 12.0\n",
      "Tópico 11 ------ 1.0\n",
      "Tópico 12 ------ 2.0\n",
      "Tópico 14 ------ 11.0\n",
      "Tópico 15 ------ 1.0\n",
      "Tópico 16 ------ 1.0\n",
      "Tópico 21 ------ 1.0\n",
      "Tópico 24 ------ 4.0\n",
      "Tópico 27 ------ 9.0\n",
      "Tópico 30 ------ 1.0\n",
      "\n",
      "Subdoença 5517\n",
      "Tópico 0 ------ 1.0\n",
      "Tópico 2 ------ 3.0\n",
      "Tópico 3 ------ 1.0\n",
      "Tópico 4 ------ 2.0\n",
      "Tópico 5 ------ 2.0\n",
      "Tópico 7 ------ 12.0\n",
      "Tópico 10 ------ 1.0\n",
      "Tópico 12 ------ 3.0\n",
      "Tópico 14 ------ 20.0\n",
      "Tópico 15 ------ 1.0\n",
      "Tópico 25 ------ 2.0\n",
      "Tópico 27 ------ 1.0\n",
      "Tópico 28 ------ 1.0\n",
      "\n",
      "Subdoença 3716\n",
      "Tópico 2 ------ 3.0\n",
      "Tópico 3 ------ 4.0\n",
      "Tópico 7 ------ 10.0\n",
      "Tópico 11 ------ 3.0\n",
      "Tópico 14 ------ 22.0\n",
      "Tópico 15 ------ 1.0\n",
      "Tópico 16 ------ 1.0\n",
      "Tópico 25 ------ 1.0\n",
      "Tópico 27 ------ 3.0\n",
      "Tópico 29 ------ 2.0\n",
      "\n",
      "Subdoença 5280\n",
      "Tópico 2 ------ 1.0\n",
      "Tópico 3 ------ 14.0\n",
      "Tópico 4 ------ 1.0\n",
      "Tópico 5 ------ 1.0\n",
      "Tópico 7 ------ 1.0\n",
      "Tópico 9 ------ 4.0\n",
      "Tópico 10 ------ 1.0\n",
      "Tópico 14 ------ 20.0\n",
      "Tópico 15 ------ 2.0\n",
      "Tópico 25 ------ 4.0\n",
      "Tópico 29 ------ 1.0\n",
      "\n",
      "Subdoença 5579\n",
      "Tópico 2 ------ 1.0\n",
      "Tópico 3 ------ 10.0\n",
      "Tópico 4 ------ 2.0\n",
      "Tópico 5 ------ 10.0\n",
      "Tópico 9 ------ 2.0\n",
      "Tópico 10 ------ 1.0\n",
      "Tópico 11 ------ 1.0\n",
      "Tópico 14 ------ 7.0\n",
      "Tópico 16 ------ 1.0\n",
      "Tópico 17 ------ 6.0\n",
      "Tópico 24 ------ 2.0\n",
      "Tópico 25 ------ 5.0\n",
      "Tópico 27 ------ 1.0\n",
      "Tópico 28 ------ 1.0\n",
      "\n",
      "Subdoença 5561\n",
      "Tópico 3 ------ 37.0\n",
      "Tópico 7 ------ 2.0\n",
      "Tópico 9 ------ 2.0\n",
      "Tópico 14 ------ 8.0\n",
      "Tópico 29 ------ 1.0\n",
      "\n",
      "Subdoença 5516\n",
      "Tópico 7 ------ 8.0\n",
      "Tópico 14 ------ 35.0\n",
      "Tópico 25 ------ 7.0\n",
      "\n",
      "Subdoença 6595\n",
      "Tópico 3 ------ 1.0\n",
      "Tópico 7 ------ 12.0\n",
      "Tópico 9 ------ 1.0\n",
      "Tópico 14 ------ 26.0\n",
      "Tópico 15 ------ 2.0\n",
      "Tópico 16 ------ 1.0\n",
      "Tópico 21 ------ 1.0\n",
      "Tópico 25 ------ 4.0\n",
      "Tópico 27 ------ 2.0\n",
      "\n",
      "Subdoença 6700\n",
      "Tópico 1 ------ 1.0\n",
      "Tópico 2 ------ 2.0\n",
      "Tópico 3 ------ 2.0\n",
      "Tópico 5 ------ 3.0\n",
      "Tópico 7 ------ 6.0\n",
      "Tópico 8 ------ 1.0\n",
      "Tópico 9 ------ 1.0\n",
      "Tópico 10 ------ 2.0\n",
      "Tópico 11 ------ 1.0\n",
      "Tópico 12 ------ 4.0\n",
      "Tópico 14 ------ 19.0\n",
      "Tópico 15 ------ 4.0\n",
      "Tópico 25 ------ 4.0\n",
      "\n",
      "Subdoença 6705\n",
      "Tópico 0 ------ 1.0\n",
      "Tópico 2 ------ 2.0\n",
      "Tópico 3 ------ 1.0\n",
      "Tópico 4 ------ 1.0\n",
      "Tópico 5 ------ 2.0\n",
      "Tópico 7 ------ 10.0\n",
      "Tópico 11 ------ 2.0\n",
      "Tópico 12 ------ 1.0\n",
      "Tópico 14 ------ 22.0\n",
      "Tópico 15 ------ 3.0\n",
      "Tópico 18 ------ 1.0\n",
      "Tópico 20 ------ 2.0\n",
      "Tópico 25 ------ 1.0\n",
      "Tópico 28 ------ 1.0\n",
      "\n",
      "Subdoença 10541\n",
      "Tópico 2 ------ 6.0\n",
      "Tópico 4 ------ 1.0\n",
      "Tópico 7 ------ 5.0\n",
      "Tópico 9 ------ 2.0\n",
      "Tópico 13 ------ 2.0\n",
      "Tópico 14 ------ 4.0\n",
      "Tópico 15 ------ 23.0\n",
      "Tópico 16 ------ 1.0\n",
      "Tópico 17 ------ 1.0\n",
      "Tópico 27 ------ 5.0\n",
      "\n",
      "Subdoença 6948\n",
      "Tópico 1 ------ 1.0\n",
      "Tópico 3 ------ 12.0\n",
      "Tópico 7 ------ 2.0\n",
      "Tópico 9 ------ 1.0\n",
      "Tópico 11 ------ 3.0\n",
      "Tópico 14 ------ 3.0\n",
      "Tópico 16 ------ 2.0\n",
      "Tópico 18 ------ 1.0\n",
      "Tópico 29 ------ 2.0\n",
      "\n",
      "Subdoença 6703\n",
      "Tópico 2 ------ 2.0\n",
      "Tópico 3 ------ 3.0\n",
      "Tópico 4 ------ 1.0\n",
      "Tópico 5 ------ 1.0\n",
      "Tópico 7 ------ 3.0\n",
      "Tópico 8 ------ 2.0\n",
      "Tópico 9 ------ 6.0\n",
      "Tópico 10 ------ 10.0\n",
      "Tópico 14 ------ 14.0\n",
      "Tópico 16 ------ 1.0\n",
      "Tópico 17 ------ 1.0\n",
      "Tópico 20 ------ 1.0\n",
      "Tópico 24 ------ 1.0\n",
      "Tópico 25 ------ 2.0\n",
      "Tópico 27 ------ 1.0\n",
      "Tópico 31 ------ 1.0\n",
      "\n",
      "Subdoença 5700\n",
      "Tópico 3 ------ 11.0\n",
      "Tópico 7 ------ 2.0\n",
      "Tópico 9 ------ 2.0\n",
      "Tópico 11 ------ 4.0\n",
      "Tópico 13 ------ 1.0\n",
      "Tópico 14 ------ 22.0\n",
      "Tópico 15 ------ 3.0\n",
      "Tópico 18 ------ 2.0\n",
      "Tópico 24 ------ 1.0\n",
      "Tópico 29 ------ 1.0\n",
      "\n",
      "Subdoença 10544\n",
      "Tópico 1 ------ 1.0\n",
      "Tópico 2 ------ 2.0\n",
      "Tópico 3 ------ 2.0\n",
      "Tópico 4 ------ 2.0\n",
      "Tópico 9 ------ 13.0\n",
      "Tópico 10 ------ 14.0\n",
      "Tópico 11 ------ 1.0\n",
      "Tópico 14 ------ 5.0\n",
      "Tópico 22 ------ 1.0\n",
      "Tópico 25 ------ 3.0\n",
      "Tópico 27 ------ 1.0\n",
      "Tópico 28 ------ 4.0\n",
      "Tópico 29 ------ 1.0\n",
      "\n",
      "Subdoença 6271\n",
      "Tópico 0 ------ 13.0\n",
      "Tópico 1 ------ 1.0\n",
      "Tópico 2 ------ 3.0\n",
      "Tópico 5 ------ 1.0\n",
      "Tópico 9 ------ 1.0\n",
      "Tópico 12 ------ 13.0\n",
      "Tópico 24 ------ 2.0\n",
      "Tópico 25 ------ 13.0\n",
      "Tópico 27 ------ 3.0\n",
      "\n",
      "Subdoença 4023\n",
      "Tópico 2 ------ 1.0\n",
      "Tópico 3 ------ 5.0\n",
      "Tópico 4 ------ 2.0\n",
      "Tópico 14 ------ 40.0\n",
      "Tópico 15 ------ 1.0\n",
      "Tópico 18 ------ 1.0\n",
      "\n",
      "Subdoença 6270\n",
      "Tópico 2 ------ 7.0\n",
      "Tópico 3 ------ 1.0\n",
      "Tópico 5 ------ 1.0\n",
      "Tópico 7 ------ 3.0\n",
      "Tópico 9 ------ 3.0\n",
      "Tópico 10 ------ 4.0\n",
      "Tópico 12 ------ 8.0\n",
      "Tópico 14 ------ 10.0\n",
      "Tópico 25 ------ 10.0\n",
      "Tópico 27 ------ 3.0\n",
      "\n",
      "Subdoença 10548\n",
      "Tópico 1 ------ 2.0\n",
      "Tópico 2 ------ 5.0\n",
      "Tópico 3 ------ 4.0\n",
      "Tópico 4 ------ 3.0\n",
      "Tópico 5 ------ 2.0\n",
      "Tópico 7 ------ 2.0\n",
      "Tópico 9 ------ 9.0\n",
      "Tópico 10 ------ 1.0\n",
      "Tópico 12 ------ 1.0\n",
      "Tópico 14 ------ 3.0\n",
      "Tópico 15 ------ 5.0\n",
      "Tópico 18 ------ 1.0\n",
      "Tópico 19 ------ 1.0\n",
      "Tópico 21 ------ 3.0\n",
      "Tópico 25 ------ 1.0\n",
      "Tópico 26 ------ 1.0\n",
      "Tópico 27 ------ 3.0\n",
      "Tópico 28 ------ 1.0\n",
      "Tópico 29 ------ 1.0\n",
      "Tópico 30 ------ 1.0\n",
      "\n",
      "Subdoença 80763\n",
      "Tópico 1 ------ 26.0\n",
      "Tópico 4 ------ 2.0\n",
      "Tópico 5 ------ 3.0\n",
      "Tópico 7 ------ 3.0\n",
      "Tópico 12 ------ 2.0\n",
      "Tópico 17 ------ 1.0\n",
      "Tópico 18 ------ 1.0\n",
      "Tópico 21 ------ 3.0\n",
      "Tópico 24 ------ 1.0\n",
      "Tópico 27 ------ 5.0\n",
      "Tópico 30 ------ 2.0\n",
      "Tópico 31 ------ 1.0\n",
      "\n",
      "Subdoença 8118\n",
      "Tópico 1 ------ 1.0\n",
      "Tópico 3 ------ 12.0\n",
      "Tópico 7 ------ 2.0\n",
      "Tópico 9 ------ 2.0\n",
      "Tópico 14 ------ 5.0\n",
      "Tópico 16 ------ 1.0\n",
      "Tópico 26 ------ 1.0\n",
      "\n",
      "Subdoença 80764\n",
      "Tópico 1 ------ 43.0\n",
      "Tópico 4 ------ 5.0\n",
      "Tópico 7 ------ 1.0\n",
      "Tópico 11 ------ 1.0\n",
      "\n",
      "Subdoença 3717\n",
      "Tópico 2 ------ 3.0\n",
      "Tópico 3 ------ 4.0\n",
      "Tópico 7 ------ 6.0\n",
      "Tópico 12 ------ 3.0\n",
      "Tópico 14 ------ 22.0\n",
      "Tópico 15 ------ 1.0\n",
      "Tópico 16 ------ 1.0\n",
      "Tópico 25 ------ 9.0\n",
      "Tópico 31 ------ 1.0\n",
      "\n",
      "Subdoença 10547\n",
      "Tópico 0 ------ 1.0\n",
      "Tópico 2 ------ 1.0\n",
      "Tópico 3 ------ 8.0\n",
      "Tópico 5 ------ 5.0\n",
      "Tópico 7 ------ 6.0\n",
      "Tópico 9 ------ 2.0\n",
      "Tópico 10 ------ 3.0\n",
      "Tópico 12 ------ 1.0\n",
      "Tópico 14 ------ 14.0\n",
      "Tópico 25 ------ 8.0\n",
      "Tópico 27 ------ 1.0\n",
      "\n",
      "Subdoença 10536\n",
      "Tópico 2 ------ 1.0\n",
      "Tópico 3 ------ 3.0\n",
      "Tópico 7 ------ 7.0\n",
      "Tópico 10 ------ 1.0\n",
      "Tópico 11 ------ 2.0\n",
      "Tópico 14 ------ 5.0\n",
      "Tópico 16 ------ 1.0\n",
      "Tópico 25 ------ 3.0\n",
      "Tópico 27 ------ 4.0\n",
      "\n",
      "Subdoença 6552\n",
      "Tópico 3 ------ 1.0\n",
      "Tópico 7 ------ 6.0\n",
      "Tópico 11 ------ 1.0\n",
      "Tópico 14 ------ 37.0\n",
      "Tópico 18 ------ 3.0\n",
      "Tópico 25 ------ 1.0\n",
      "Tópico 29 ------ 1.0\n",
      "\n",
      "Subdoença 5635\n",
      "Tópico 2 ------ 1.0\n",
      "Tópico 3 ------ 1.0\n",
      "Tópico 4 ------ 1.0\n",
      "Tópico 7 ------ 4.0\n",
      "Tópico 14 ------ 35.0\n",
      "Tópico 18 ------ 2.0\n",
      "Tópico 24 ------ 3.0\n",
      "Tópico 26 ------ 1.0\n",
      "Tópico 27 ------ 1.0\n",
      "Tópico 30 ------ 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda_validation(doids, pubs, doc_topic_dist)"
   ]
  },
  {
   "source": [
    "Visto isto, fomos analisar quais era as principais componentes do tópico 14 e como podemos verificar através da função seguinte são as palavras: \"gastric\", \"case\", \"tumor\", \"carcinoma\", \"stomach\", \"node\", \"adenocarcinoma\", \"reveal\", \"metastas\", entre outras."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Topic #0: risk ci cancer associ gastric studi factor ratio increas use interv confid age odd smoke adjust patient year control rr cohort subject signific hr regress\n",
      "\n",
      "Topic #1: cancer mutat gastric gene polymorph associ genotyp studi genet famili risk allel variant ci patient analysi found result sequenc control suscept dna identifi snp case\n",
      "\n",
      "Topic #2: patient node cancer gastric lymph metastasi tumor invas surviv resect rate factor recurr stage type earli prognosi advanc size cur egc studi clinicopatholog dissect depth\n",
      "\n",
      "Topic #3: patient tumor year case gastrointestin gist age present diagnosi tumour symptom diseas malign clinic month gi treatment primari report tract small stromal surgeri gastric stomach\n",
      "\n",
      "Topic #4: cancer treatment gastric clinic review diseas studi therapi use trial improv develop patient current includ recent provid research approach new advanc strategi result therapeut data\n",
      "\n",
      "Topic #5: gastric patient gastriti cancer serum level pylori metaplasia mucosa intestin chronic atroph subject infect studi higher biopsi control atrophi significantli dysplasia lesion test group posit\n",
      "\n",
      "Topic #6: laparoscop gastrectomi open vegf crc use time gastric compar studi perform cancer outcom oper total surgeri robot minim ladg feasibl surgic distal loss blood invas\n",
      "\n",
      "Topic #7: lymphoma type case cell gastric intestin carcinoma histolog stomach malt diffus mucosa tumour tissu gland lesion lymphoid mucin pattern differenti adenocarcinoma larg studi found malign\n",
      "\n",
      "Topic #8: group patient level control blood significantli cancer differ signific serum higher compar plasma gastric studi lower valu measur increas decreas correl concentr nutrit chang preoper\n",
      "\n",
      "Topic #9: patient gastric case perform resect use oper arteri complic stomach esophag surgeri procedur pancreat tube surgic reconstruct anastomosi left techniqu obstruct cancer abdomin tumor esophagectomi\n",
      "\n",
      "Topic #10: patient group gastrectomi postop resect gastric complic surgeri rate oper underw total surgic esd hospit time compar studi mortal cancer outcom procedur differ morbid day\n",
      "\n",
      "Topic #11: cancer lung breast malign pancreat patient tumor carcinoma primari diseas prostat ovarian stomach colon bladder pancrea case thyroid renal kidney liver includ organ melanoma skin\n",
      "\n",
      "Topic #12: cancer rate incid mortal year stomach increas death age popul studi data women men male femal lung period countri case estim trend use differ screen\n",
      "\n",
      "Topic #13: drug activ use effect cancer studi acid gastric human compound tissu result agent tumor model concentr vivo vitro anticanc potenti enzym inhibit sensit protein antitumor\n",
      "\n",
      "Topic #14: gastric case tumor carcinoma node adenocarcinoma reveal metastas report cell lymph metastasi stomach lesion cancer metastat examin primari present rare diagnos diagnosi gastrectomi man biopsi\n",
      "\n",
      "Topic #15: endoscop gastric lesion use imag detect diagnosi ct biopsi method diagnost endoscopi cancer studi evalu accuraci earli patient sensit eu perform examin specif mm submucos\n",
      "\n",
      "Topic #16: antibodi antigen cell tumor monoclon use cancer human specif normal tissu immun peptid bind carcinoma mab protein mucin studi colon detect stain gastric reactiv express\n",
      "\n",
      "Topic #17: gastric cell activ respons role factor signal develop receptor pathway induc function mechan acid immun secret epitheli cancer increas human inflamm protein regul gastrin inflammatori\n",
      "\n",
      "Topic #18: patient chemotherapi treatment respons cancer gastric surviv month therapi advanc combin day receiv dose median toxic treat rate effect regimen studi grade adjuv cisplatin week\n",
      "\n",
      "Topic #19: periton cancer patient cea gastric dissemin cytolog ascit intraperiton use posit detect level fluid carcinomatosi sensit case ts marker recurr result malign concentr laparoscopi metastasi\n",
      "\n",
      "Topic #20: rat studi stomach intak effect week increas mice dose male diet food group anim carcinogen incid vitamin femal control dietari water exposur observ weight forestomach\n",
      "\n",
      "Topic #21: gc gene express methyl cancer gastric tissu analysi mirna promot identifi use biomark rna lncrna studi associ dna level potenti sampl target patient role tumor\n",
      "\n",
      "Topic #22: cell apoptosi effect activ gastric cancer inhibit induc increas treatment prolifer human line growth express use result studi ag assay decreas flow protein significantli cytotox\n",
      "\n",
      "Topic #23: cancer esophag colorect colon squamou gastrointestin cell gastric escc pc increas fa rectal tissu normal eac carcinoma treg studi esophagu obes sc compar bc tumor\n",
      "\n",
      "Topic #24: patient surviv stage prognost analysi overal gastric os factor predict independ cancer multivari studi use prognosi hr outcom associ ratio clinic signific tumor compar model\n",
      "\n",
      "Topic #25: adenocarcinoma tumor polyp esophagu adenoma gastric stomach gastrointestin barrett lesion carcinoma tract junction syndrom cardia malign dysplasia neoplasm polyposi develop report reflux duoden colon hyperplast\n",
      "\n",
      "Topic #26: liver hepat tumor metastas msi hcc microsatellit instabl hepatocellular afp cancer level bone arteri serum gastric patient cirrhosi carcinoma case multipl hepatectomi metastat portal ablat\n",
      "\n",
      "Topic #27: express gastric tissu tumor cancer carcinoma correl protein posit normal significantli associ mrna use case level patient studi adenocarcinoma immunohistochemistri signific differenti stain higher sampl\n",
      "\n",
      "Topic #28: ulcer pylori erad gastric patient stent therapi treatment infect effect peptic helicobact duoden ppi hp use reflux diseas antibiot success test studi proton prevent symptom\n",
      "\n",
      "Topic #29: cell tumor line human mice gene express cancer growth carcinoma gastric nude model use activ mous clone protein cultur mrna tissu establish stem telomeras observ\n",
      "\n",
      "Topic #30: cell cancer gastric express inhibit prolifer invas target protein tumor migrat studi growth role effect line assay activ overexpress pathway signal regul promot level suppress\n",
      "\n",
      "Topic #31: pylori infect helicobact gastric strain caga diseas associ ulcer factor gene studi isol bacteri vaca cancer gastriti develop pathogen host differ preval patient virul peptic\n"
     ]
    }
   ],
   "source": [
    "print_top_words(lda, vectorizer, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      Word  Frequency\n",
      "0  gastric     257759\n",
      "    Word  Frequency\n",
      "94  case      16335\n",
      "    Word  Frequency\n",
      "5  tumor      69397\n",
      "         Word  Frequency\n",
      "16  carcinoma      38448\n",
      "       Word  Frequency\n",
      "13  stomach      40408\n",
      "    Word  Frequency\n",
      "64  node      20697\n",
      "              Word  Frequency\n",
      "51  adenocarcinoma      22552\n",
      "        Word  Frequency\n",
      "1681  reveal       1155\n"
     ]
    }
   ],
   "source": [
    "word_count = pd.read_csv('data/word_count.csv', sep=',')\n",
    "print(word_count[word_count['Word']=='gastric'])\n",
    "print(word_count[word_count['Word']=='case'])\n",
    "print(word_count[word_count['Word']=='tumor'])\n",
    "print(word_count[word_count['Word']=='carcinoma'])\n",
    "print(word_count[word_count['Word']=='stomach'])\n",
    "print(word_count[word_count['Word']=='node'])\n",
    "print(word_count[word_count['Word']=='adenocarcinoma'])\n",
    "print(word_count[word_count['Word']=='reveal'])"
   ]
  },
  {
   "source": [
    "Como estas palavras se encontram em grande parte das publicações, elas estão a ser associadas ao tópico 14. Posto isto é importante evitar que existam palavras muito frequentes que não permitam ao LDA ter um bom desempenho"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Modelo 2\n",
    "\n",
    "Para este modelo usamos os dataset clean.csv adicionando as palavras em que a sua frequencia é superior a 1 às stop_words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lda(df,'modelo2',True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\jpmrs\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:310: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.24.2 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jpmrs\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:310: UserWarning: Trying to unpickle estimator LatentDirichletAllocation from version 0.24.2 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vectorizer = joblib.load('modelo2/vectorizer.csv')\n",
    "data_vectorized = joblib.load('modelo2/data_vectorized.csv')\n",
    "lda = joblib.load('modelo2/lda.csv') \n",
    "doc_topic_dist = pd.read_csv('modelo2/doc_topic_dist.csv')  "
   ]
  },
  {
   "source": [
    "De seguida, deixo exemplos da distribuição das publicações das subdoenças 0, 1 e 25 pelos diveros tópicos"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Tab(children=(Output(), Output(), Output(), Output(), Output(), Output(), Output(), Output(), Output(), Output…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "da14c5c2fed84dabb0b90baed8103565"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "#documentos da sub doença 0\n",
    "papers=pubs[pubs[\"doid\"]==doids[0]]\n",
    "\n",
    "lista=[]\n",
    "[lista.append(papers.iloc[i].id) for i in range(1,len(papers))]\n",
    "\n",
    "dna_tabs(lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Tab(children=(Output(), Output(), Output(), Output(), Output(), Output(), Output(), Output(), Output(), Output…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4dd87abeeb414942994a7413a44fb281"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "#documentos da sub doença 1\n",
    "papers=pubs[pubs[\"doid\"]==doids[1]]\n",
    "\n",
    "lista=[]\n",
    "[lista.append(papers.iloc[i].id) for i in range(1,len(papers))]\n",
    "\n",
    "dna_tabs(lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Tab(children=(Output(), Output(), Output(), Output(), Output(), Output(), Output(), Output(), Output(), Output…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ffd2b9ae8f034fa29d2a063fd1a33cc9"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "#documentos da sub doença 25\n",
    "papers=pubs[pubs[\"doid\"]==doids[25]]\n",
    "\n",
    "lista=[]\n",
    "[lista.append(papers.iloc[i].id) for i in range(1,len(papers))]\n",
    "\n",
    "dna_tabs(lista)"
   ]
  },
  {
   "source": [
    "Analisando os gráficos para estas três subdoenças podemos perceber que nem todas as publicações de uma mesma sub-doença estão associadas a um tópico. No entanto podemos notar que existe uma tendencia como é o caso do tópico 17 para a doença 0, o tópico 7 para a subdoença 1 e  o tóp\n",
    "Porém, analisando melhor a distribuição da associação das publicaçoes ás sub-doenças podemos reparar através dos resultados da função seguinte que maioria dos documentos de todas as subdoenças estão a ser associados ao tópico 20."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Subdoença 4716\n",
      "Tópico 0 ------ 1.0\n",
      "Tópico 1 ------ 2.0\n",
      "Tópico 3 ------ 2.0\n",
      "Tópico 4 ------ 3.0\n",
      "Tópico 6 ------ 7.0\n",
      "Tópico 7 ------ 7.0\n",
      "Tópico 8 ------ 1.0\n",
      "Tópico 10 ------ 1.0\n",
      "Tópico 11 ------ 1.0\n",
      "Tópico 17 ------ 13.0\n",
      "Tópico 18 ------ 1.0\n",
      "Tópico 19 ------ 1.0\n",
      "Tópico 20 ------ 7.0\n",
      "Tópico 27 ------ 1.0\n",
      "Tópico 29 ------ 1.0\n",
      "Tópico 30 ------ 1.0\n",
      "\n",
      "Subdoença 8025\n",
      "Tópico 0 ------ 5.0\n",
      "Tópico 2 ------ 1.0\n",
      "Tópico 6 ------ 1.0\n",
      "Tópico 7 ------ 15.0\n",
      "Tópico 11 ------ 6.0\n",
      "Tópico 12 ------ 2.0\n",
      "Tópico 17 ------ 1.0\n",
      "Tópico 18 ------ 4.0\n",
      "Tópico 20 ------ 13.0\n",
      "Tópico 23 ------ 1.0\n",
      "Tópico 27 ------ 1.0\n",
      "\n",
      "Subdoença 10538\n",
      "Tópico 0 ------ 6.0\n",
      "Tópico 1 ------ 1.0\n",
      "Tópico 4 ------ 1.0\n",
      "Tópico 5 ------ 2.0\n",
      "Tópico 7 ------ 8.0\n",
      "Tópico 12 ------ 2.0\n",
      "Tópico 15 ------ 1.0\n",
      "Tópico 17 ------ 1.0\n",
      "Tópico 18 ------ 4.0\n",
      "Tópico 20 ------ 7.0\n",
      "Tópico 23 ------ 1.0\n",
      "Tópico 24 ------ 6.0\n",
      "Tópico 27 ------ 1.0\n",
      "Tópico 29 ------ 1.0\n",
      "Tópico 30 ------ 3.0\n",
      "Tópico 31 ------ 4.0\n",
      "\n",
      "Subdoença 10540\n",
      "Tópico 0 ------ 8.0\n",
      "Tópico 5 ------ 1.0\n",
      "Tópico 7 ------ 20.0\n",
      "Tópico 12 ------ 1.0\n",
      "Tópico 13 ------ 5.0\n",
      "Tópico 16 ------ 1.0\n",
      "Tópico 17 ------ 1.0\n",
      "Tópico 18 ------ 5.0\n",
      "Tópico 19 ------ 1.0\n",
      "Tópico 20 ------ 1.0\n",
      "Tópico 24 ------ 2.0\n",
      "Tópico 28 ------ 1.0\n",
      "Tópico 30 ------ 3.0\n",
      "\n",
      "Subdoença 5593\n",
      "Tópico 0 ------ 2.0\n",
      "Tópico 3 ------ 1.0\n",
      "Tópico 5 ------ 2.0\n",
      "Tópico 6 ------ 1.0\n",
      "Tópico 7 ------ 2.0\n",
      "Tópico 11 ------ 3.0\n",
      "Tópico 17 ------ 2.0\n",
      "Tópico 18 ------ 9.0\n",
      "Tópico 19 ------ 1.0\n",
      "Tópico 20 ------ 23.0\n",
      "Tópico 24 ------ 1.0\n",
      "Tópico 27 ------ 1.0\n",
      "Tópico 28 ------ 1.0\n",
      "Tópico 31 ------ 1.0\n",
      "\n",
      "Subdoença 6217\n",
      "Tópico 0 ------ 5.0\n",
      "Tópico 2 ------ 1.0\n",
      "Tópico 4 ------ 1.0\n",
      "Tópico 5 ------ 1.0\n",
      "Tópico 7 ------ 11.0\n",
      "Tópico 8 ------ 1.0\n",
      "Tópico 11 ------ 15.0\n",
      "Tópico 13 ------ 1.0\n",
      "Tópico 17 ------ 3.0\n",
      "Tópico 18 ------ 2.0\n",
      "Tópico 21 ------ 1.0\n",
      "Tópico 24 ------ 6.0\n",
      "Tópico 25 ------ 1.0\n",
      "Tópico 28 ------ 1.0\n",
      "\n",
      "Subdoença 5517\n",
      "Tópico 0 ------ 8.0\n",
      "Tópico 4 ------ 2.0\n",
      "Tópico 5 ------ 1.0\n",
      "Tópico 7 ------ 6.0\n",
      "Tópico 8 ------ 1.0\n",
      "Tópico 10 ------ 1.0\n",
      "Tópico 11 ------ 1.0\n",
      "Tópico 16 ------ 2.0\n",
      "Tópico 17 ------ 2.0\n",
      "Tópico 18 ------ 6.0\n",
      "Tópico 20 ------ 12.0\n",
      "Tópico 24 ------ 4.0\n",
      "Tópico 25 ------ 1.0\n",
      "Tópico 26 ------ 1.0\n",
      "Tópico 27 ------ 1.0\n",
      "Tópico 30 ------ 1.0\n",
      "\n",
      "Subdoença 3716\n",
      "Tópico 0 ------ 6.0\n",
      "Tópico 3 ------ 2.0\n",
      "Tópico 5 ------ 1.0\n",
      "Tópico 6 ------ 3.0\n",
      "Tópico 7 ------ 20.0\n",
      "Tópico 11 ------ 2.0\n",
      "Tópico 17 ------ 1.0\n",
      "Tópico 18 ------ 3.0\n",
      "Tópico 20 ------ 8.0\n",
      "Tópico 23 ------ 2.0\n",
      "Tópico 26 ------ 1.0\n",
      "Tópico 30 ------ 1.0\n",
      "\n",
      "Subdoença 5280\n",
      "Tópico 0 ------ 15.0\n",
      "Tópico 2 ------ 1.0\n",
      "Tópico 7 ------ 15.0\n",
      "Tópico 10 ------ 1.0\n",
      "Tópico 11 ------ 1.0\n",
      "Tópico 12 ------ 2.0\n",
      "Tópico 16 ------ 2.0\n",
      "Tópico 17 ------ 1.0\n",
      "Tópico 18 ------ 2.0\n",
      "Tópico 19 ------ 1.0\n",
      "Tópico 20 ------ 2.0\n",
      "Tópico 27 ------ 1.0\n",
      "Tópico 30 ------ 5.0\n",
      "Tópico 31 ------ 1.0\n",
      "\n",
      "Subdoença 5579\n",
      "Tópico 0 ------ 2.0\n",
      "Tópico 5 ------ 2.0\n",
      "Tópico 6 ------ 1.0\n",
      "Tópico 7 ------ 8.0\n",
      "Tópico 9 ------ 1.0\n",
      "Tópico 12 ------ 1.0\n",
      "Tópico 15 ------ 4.0\n",
      "Tópico 16 ------ 4.0\n",
      "Tópico 18 ------ 1.0\n",
      "Tópico 19 ------ 1.0\n",
      "Tópico 20 ------ 12.0\n",
      "Tópico 22 ------ 1.0\n",
      "Tópico 24 ------ 9.0\n",
      "Tópico 30 ------ 3.0\n",
      "\n",
      "Subdoença 5561\n",
      "Tópico 0 ------ 8.0\n",
      "Tópico 6 ------ 1.0\n",
      "Tópico 7 ------ 26.0\n",
      "Tópico 15 ------ 1.0\n",
      "Tópico 16 ------ 1.0\n",
      "Tópico 18 ------ 2.0\n",
      "Tópico 20 ------ 7.0\n",
      "Tópico 22 ------ 1.0\n",
      "Tópico 27 ------ 1.0\n",
      "Tópico 30 ------ 2.0\n",
      "\n",
      "Subdoença 5516\n",
      "Tópico 0 ------ 17.0\n",
      "Tópico 4 ------ 1.0\n",
      "Tópico 6 ------ 1.0\n",
      "Tópico 7 ------ 16.0\n",
      "Tópico 18 ------ 2.0\n",
      "Tópico 20 ------ 7.0\n",
      "Tópico 24 ------ 1.0\n",
      "Tópico 28 ------ 3.0\n",
      "Tópico 30 ------ 1.0\n",
      "Tópico 31 ------ 1.0\n",
      "\n",
      "Subdoença 6595\n",
      "Tópico 0 ------ 7.0\n",
      "Tópico 3 ------ 1.0\n",
      "Tópico 7 ------ 5.0\n",
      "Tópico 10 ------ 1.0\n",
      "Tópico 11 ------ 3.0\n",
      "Tópico 17 ------ 2.0\n",
      "Tópico 18 ------ 7.0\n",
      "Tópico 20 ------ 23.0\n",
      "Tópico 24 ------ 1.0\n",
      "\n",
      "Subdoença 6700\n",
      "Tópico 0 ------ 7.0\n",
      "Tópico 4 ------ 2.0\n",
      "Tópico 5 ------ 1.0\n",
      "Tópico 7 ------ 5.0\n",
      "Tópico 12 ------ 2.0\n",
      "Tópico 15 ------ 2.0\n",
      "Tópico 16 ------ 1.0\n",
      "Tópico 17 ------ 2.0\n",
      "Tópico 18 ------ 5.0\n",
      "Tópico 20 ------ 12.0\n",
      "Tópico 24 ------ 3.0\n",
      "Tópico 26 ------ 1.0\n",
      "Tópico 27 ------ 1.0\n",
      "Tópico 30 ------ 4.0\n",
      "Tópico 31 ------ 2.0\n",
      "\n",
      "Subdoença 6705\n",
      "Tópico 0 ------ 7.0\n",
      "Tópico 1 ------ 1.0\n",
      "Tópico 4 ------ 2.0\n",
      "Tópico 5 ------ 2.0\n",
      "Tópico 7 ------ 3.0\n",
      "Tópico 11 ------ 1.0\n",
      "Tópico 12 ------ 2.0\n",
      "Tópico 13 ------ 3.0\n",
      "Tópico 18 ------ 2.0\n",
      "Tópico 20 ------ 13.0\n",
      "Tópico 24 ------ 3.0\n",
      "Tópico 25 ------ 2.0\n",
      "Tópico 26 ------ 2.0\n",
      "Tópico 27 ------ 1.0\n",
      "Tópico 29 ------ 1.0\n",
      "Tópico 30 ------ 5.0\n",
      "\n",
      "Subdoença 10541\n",
      "Tópico 0 ------ 6.0\n",
      "Tópico 2 ------ 4.0\n",
      "Tópico 3 ------ 3.0\n",
      "Tópico 12 ------ 2.0\n",
      "Tópico 15 ------ 1.0\n",
      "Tópico 16 ------ 1.0\n",
      "Tópico 18 ------ 26.0\n",
      "Tópico 20 ------ 2.0\n",
      "Tópico 24 ------ 1.0\n",
      "Tópico 28 ------ 2.0\n",
      "Tópico 30 ------ 2.0\n",
      "\n",
      "Subdoença 6948\n",
      "Tópico 0 ------ 1.0\n",
      "Tópico 6 ------ 2.0\n",
      "Tópico 7 ------ 6.0\n",
      "Tópico 13 ------ 1.0\n",
      "Tópico 16 ------ 1.0\n",
      "Tópico 17 ------ 1.0\n",
      "Tópico 18 ------ 1.0\n",
      "Tópico 19 ------ 1.0\n",
      "Tópico 20 ------ 8.0\n",
      "Tópico 22 ------ 1.0\n",
      "Tópico 27 ------ 1.0\n",
      "Tópico 30 ------ 3.0\n",
      "\n",
      "Subdoença 6703\n",
      "Tópico 0 ------ 3.0\n",
      "Tópico 5 ------ 1.0\n",
      "Tópico 7 ------ 4.0\n",
      "Tópico 11 ------ 1.0\n",
      "Tópico 15 ------ 3.0\n",
      "Tópico 18 ------ 8.0\n",
      "Tópico 20 ------ 9.0\n",
      "Tópico 24 ------ 4.0\n",
      "Tópico 26 ------ 2.0\n",
      "Tópico 27 ------ 1.0\n",
      "Tópico 30 ------ 6.0\n",
      "Tópico 31 ------ 8.0\n",
      "\n",
      "Subdoença 5700\n",
      "Tópico 0 ------ 6.0\n",
      "Tópico 5 ------ 1.0\n",
      "Tópico 7 ------ 12.0\n",
      "Tópico 8 ------ 1.0\n",
      "Tópico 12 ------ 1.0\n",
      "Tópico 13 ------ 1.0\n",
      "Tópico 16 ------ 2.0\n",
      "Tópico 18 ------ 2.0\n",
      "Tópico 19 ------ 3.0\n",
      "Tópico 20 ------ 9.0\n",
      "Tópico 26 ------ 1.0\n",
      "Tópico 30 ------ 8.0\n",
      "Tópico 31 ------ 2.0\n",
      "\n",
      "Subdoença 10544\n",
      "Tópico 0 ------ 3.0\n",
      "Tópico 5 ------ 2.0\n",
      "Tópico 6 ------ 1.0\n",
      "Tópico 7 ------ 2.0\n",
      "Tópico 12 ------ 1.0\n",
      "Tópico 16 ------ 1.0\n",
      "Tópico 17 ------ 1.0\n",
      "Tópico 18 ------ 3.0\n",
      "Tópico 20 ------ 2.0\n",
      "Tópico 21 ------ 1.0\n",
      "Tópico 24 ------ 2.0\n",
      "Tópico 27 ------ 3.0\n",
      "Tópico 29 ------ 1.0\n",
      "Tópico 30 ------ 14.0\n",
      "Tópico 31 ------ 13.0\n",
      "\n",
      "Subdoença 6271\n",
      "Tópico 0 ------ 3.0\n",
      "Tópico 3 ------ 2.0\n",
      "Tópico 4 ------ 14.0\n",
      "Tópico 5 ------ 4.0\n",
      "Tópico 7 ------ 1.0\n",
      "Tópico 8 ------ 4.0\n",
      "Tópico 11 ------ 2.0\n",
      "Tópico 17 ------ 1.0\n",
      "Tópico 21 ------ 2.0\n",
      "Tópico 24 ------ 4.0\n",
      "Tópico 25 ------ 7.0\n",
      "Tópico 28 ------ 5.0\n",
      "Tópico 31 ------ 1.0\n",
      "\n",
      "Subdoença 4023\n",
      "Tópico 0 ------ 7.0\n",
      "Tópico 7 ------ 38.0\n",
      "Tópico 12 ------ 1.0\n",
      "Tópico 18 ------ 1.0\n",
      "Tópico 20 ------ 1.0\n",
      "Tópico 27 ------ 1.0\n",
      "Tópico 31 ------ 1.0\n",
      "\n",
      "Subdoença 6270\n",
      "Tópico 0 ------ 5.0\n",
      "Tópico 3 ------ 2.0\n",
      "Tópico 4 ------ 9.0\n",
      "Tópico 5 ------ 4.0\n",
      "Tópico 11 ------ 5.0\n",
      "Tópico 15 ------ 1.0\n",
      "Tópico 16 ------ 1.0\n",
      "Tópico 17 ------ 1.0\n",
      "Tópico 18 ------ 4.0\n",
      "Tópico 20 ------ 3.0\n",
      "Tópico 24 ------ 1.0\n",
      "Tópico 28 ------ 4.0\n",
      "Tópico 30 ------ 5.0\n",
      "Tópico 31 ------ 5.0\n",
      "\n",
      "Subdoença 10548\n",
      "Tópico 0 ------ 5.0\n",
      "Tópico 2 ------ 3.0\n",
      "Tópico 3 ------ 2.0\n",
      "Tópico 4 ------ 1.0\n",
      "Tópico 5 ------ 4.0\n",
      "Tópico 7 ------ 6.0\n",
      "Tópico 8 ------ 2.0\n",
      "Tópico 11 ------ 5.0\n",
      "Tópico 12 ------ 3.0\n",
      "Tópico 18 ------ 2.0\n",
      "Tópico 19 ------ 1.0\n",
      "Tópico 20 ------ 4.0\n",
      "Tópico 23 ------ 1.0\n",
      "Tópico 24 ------ 1.0\n",
      "Tópico 30 ------ 3.0\n",
      "Tópico 31 ------ 7.0\n",
      "\n",
      "Subdoença 80763\n",
      "Tópico 1 ------ 1.0\n",
      "Tópico 2 ------ 2.0\n",
      "Tópico 3 ------ 1.0\n",
      "Tópico 4 ------ 1.0\n",
      "Tópico 5 ------ 2.0\n",
      "Tópico 8 ------ 5.0\n",
      "Tópico 10 ------ 1.0\n",
      "Tópico 11 ------ 7.0\n",
      "Tópico 12 ------ 1.0\n",
      "Tópico 16 ------ 1.0\n",
      "Tópico 17 ------ 23.0\n",
      "Tópico 24 ------ 4.0\n",
      "Tópico 28 ------ 1.0\n",
      "\n",
      "Subdoença 8118\n",
      "Tópico 0 ------ 1.0\n",
      "Tópico 6 ------ 1.0\n",
      "Tópico 7 ------ 9.0\n",
      "Tópico 14 ------ 1.0\n",
      "Tópico 17 ------ 1.0\n",
      "Tópico 20 ------ 7.0\n",
      "Tópico 27 ------ 2.0\n",
      "Tópico 30 ------ 2.0\n",
      "\n",
      "Subdoença 80764\n",
      "Tópico 0 ------ 1.0\n",
      "Tópico 16 ------ 6.0\n",
      "Tópico 17 ------ 42.0\n",
      "Tópico 28 ------ 1.0\n",
      "\n",
      "Subdoença 3717\n",
      "Tópico 0 ------ 5.0\n",
      "Tópico 4 ------ 3.0\n",
      "Tópico 6 ------ 2.0\n",
      "Tópico 7 ------ 12.0\n",
      "Tópico 11 ------ 4.0\n",
      "Tópico 12 ------ 1.0\n",
      "Tópico 15 ------ 1.0\n",
      "Tópico 16 ------ 1.0\n",
      "Tópico 17 ------ 2.0\n",
      "Tópico 18 ------ 2.0\n",
      "Tópico 20 ------ 10.0\n",
      "Tópico 22 ------ 1.0\n",
      "Tópico 24 ------ 2.0\n",
      "Tópico 27 ------ 1.0\n",
      "Tópico 28 ------ 1.0\n",
      "Tópico 30 ------ 2.0\n",
      "\n",
      "Subdoença 10547\n",
      "Tópico 0 ------ 5.0\n",
      "Tópico 4 ------ 2.0\n",
      "Tópico 7 ------ 9.0\n",
      "Tópico 8 ------ 1.0\n",
      "Tópico 11 ------ 2.0\n",
      "Tópico 13 ------ 1.0\n",
      "Tópico 16 ------ 2.0\n",
      "Tópico 18 ------ 2.0\n",
      "Tópico 19 ------ 2.0\n",
      "Tópico 20 ------ 7.0\n",
      "Tópico 24 ------ 6.0\n",
      "Tópico 25 ------ 2.0\n",
      "Tópico 26 ------ 1.0\n",
      "Tópico 27 ------ 2.0\n",
      "Tópico 30 ------ 6.0\n",
      "\n",
      "Subdoença 10536\n",
      "Tópico 0 ------ 3.0\n",
      "Tópico 3 ------ 1.0\n",
      "Tópico 6 ------ 2.0\n",
      "Tópico 7 ------ 5.0\n",
      "Tópico 11 ------ 6.0\n",
      "Tópico 16 ------ 1.0\n",
      "Tópico 18 ------ 2.0\n",
      "Tópico 20 ------ 5.0\n",
      "Tópico 30 ------ 1.0\n",
      "Tópico 31 ------ 1.0\n",
      "\n",
      "Subdoença 6552\n",
      "Tópico 0 ------ 3.0\n",
      "Tópico 3 ------ 1.0\n",
      "Tópico 5 ------ 2.0\n",
      "Tópico 7 ------ 10.0\n",
      "Tópico 13 ------ 2.0\n",
      "Tópico 19 ------ 5.0\n",
      "Tópico 20 ------ 24.0\n",
      "Tópico 30 ------ 3.0\n",
      "\n",
      "Subdoença 5635\n",
      "Tópico 0 ------ 20.0\n",
      "Tópico 5 ------ 4.0\n",
      "Tópico 6 ------ 1.0\n",
      "Tópico 7 ------ 4.0\n",
      "Tópico 9 ------ 1.0\n",
      "Tópico 11 ------ 1.0\n",
      "Tópico 12 ------ 1.0\n",
      "Tópico 16 ------ 1.0\n",
      "Tópico 17 ------ 1.0\n",
      "Tópico 19 ------ 5.0\n",
      "Tópico 20 ------ 7.0\n",
      "Tópico 24 ------ 2.0\n",
      "Tópico 30 ------ 2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda_validation(doids, pubs, doc_topic_dist)"
   ]
  },
  {
   "source": [
    "Este mau resultado do LDA pode ser explicado porque o LDA apenas foi treinado com palavras que apareceram uma vez e daí nao é possível concluir quais são os tópicos de cada componente"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTopic #0: biopsi diagnosi endoscop examin endoscopi diagnos histolog esophag esophagu patholog diagnost synchron tomographi metachron comput evalu ultrasonographi cytolog underw studi preoper gastrointestin histopatholog aspir varic\n\nTopic #1: promot regul mechan studi prolifer tissu upregul involv associ downregul potenti carcinogenesi howev demonstr tumorigenesi identifi silenc indic therapeut hypermethyl epigenet correl addit furthermor underli\n\nTopic #2: studi analysi identifi sampl associ databas includ profil potenti amplif systemat compar valu sensit evalu differenti specif analys analyz pubm heterogen prognost signatur curv embas\n\nTopic #3: tissu correl significantli sampl compar adjac studi signific associ metastasi analysi investig immunohistochemistri prognosi analyz surviv indic differenti clinicopatholog evalu determin increas immunohistochem quantit healthi\n\nTopic #4: incid increas femal studi observ countri decreas compar registri estim prostat significantli declin gener caus signific epidemiolog statist colorect calcul averag diagnos includ esophagu geograph\n\nTopic #5: surviv studi surgeri compar analysi recurr outcom chemotherapi significantli prognost signific underw multivari gastrectomi independ adjuv preoper prognosi surgic associ receiv improv advanc evalu includ\n\nTopic #6: pancreat antibodi pancrea monoclon tissu prostat colorect specif studi cervic epitheli reactiv includ gallbladd membran exosom observ immunoreact variou demonstr squamou elev biliari immunohistochem ovari\n\nTopic #7: primari diagnosi diseas gastrointestin involv associ multipl pulmonari describ literatur secondari diagnos acut histori caus featur examin abdomin includ initi diffus histolog tissu metastas unusu\n\nTopic #8: associ genotyp studi famili genet allel frequenc suscept increas individu signific significantli histori compar investig analysi healthi determin singl observ howev interv confid includ nucleotid\n\nTopic #9: invas migrat metastasi significantli prolifer angiogenesi studi adhes endotheli increas abil investig lymphat decreas activ reduc transwel associ telomeras densiti examin promot correl potenti howev\n\nTopic #10: apoptosi prolifer induc increas studi activ cycl decreas significantli mechan investig apoptot potenti viabil cytometri analysi indic enhanc demonstr reduc determin autophagi compar sensit howev\n\nTopic #11: prognost correl surviv prognosi histolog classif signific associ clinicopatholog studi analysi featur independ significantli evalu statu diffus advanc subtyp paramet multivari valu infiltr intestin invas\n\nTopic #12: imag metastas sensit valu accuraci specif evalu diagnost studi compar tomographi comput preoper uptak volum techniqu primari determin enhanc convent diagnosi purpos measur accur improv\n\nTopic #13: therapi radiat radiotherapi chemotherapi diseas combin improv receiv complet adjuv antibiot remiss failur howev achiev initi advanc recurr irradi tripl consid helicobact palli manag studi\n\nTopic #14: hepat arteri exposur nitrit embol nitrat venou occup cirrhosi hypertens suppli caus coronari expos angiographi alkalin asbesto cathet mesenter industri nitros extrahepat thrombosi studi elev\n\nTopic #15: diseas caus helicobact metabol associ disord defici bacteri diabet glucos studi gastroduoden increas duoden includ virul relat pathogenesi infecti condit gastrectomi possibl mechan hormon evid\n\nTopic #16: gastrointestin manag diagnosi diseas practic diagnost studi endoscopi includ provid articl improv colorect guidelin earli avail strategi qualiti literatur evid gener surveil evalu requir consid\n\nTopic #17: mutat sequenc colorect chromosom genet microsatellit famili instabl identifi delet analysi hereditari germlin observ includ sporad amplif somat aberr studi diffus analyz primari frequenc involv\n\nTopic #18: endoscop earli submucos mucos endoscopi examin procedur studi perfor locat histolog complet indic invas diamet techniqu superfici remov observ evalu howev intramucos diagnosi underw diagnos\n\nTopic #19: chemotherapi respons advanc combin surviv studi receiv efficaci evalu advers administ complet diseas cycl infus cours observ safeti toler administr everi therapi includ continu primari\n\nTopic #20: metastasi differenti metastas poorli neuroendocrin histolog invas featur lymphat endocrin immunohistochem primari compon locat undifferenti patholog involv examin gastrointestin moder characterist morpholog multipl observ studi\n\nTopic #21: hospit studi associ medic compar qualiti surgeri includ increas outcom significantli bodi questionnair univers statu signific particip colorect diagnos identifi measur physic evalu diagnosi analysi\n\nTopic #22: immun respons lymphocyt increas activ induc cultur inflammatori studi cytokin isol cytotox peripher epitheli stimul inflamm macrophag produc specif releas surfac damag structur helicobact vaccin\n\nTopic #23: periton dissemin metastasi intraperiton cytolog muscl carcinomatosi caviti recurr cytoreduct lavag seros ascit hypertherm skelet examin studi nodul laparoscopi prognosi abdomin metastas mesotheli glycolysi macroscop\n\nTopic #24: intestin gastriti helicobact atroph studi associ atrophi histolog biopsi mucos increas inflamm presenc corpu examin carcinogenesi significantli epitheli compar determin observ signific investig evalu relat\n\nTopic #25: studi associ interv intak increas confid estim consumpt signific dietari versu compar evid reduc observ particip includ veget significantli baselin decreas statist relat exposur invers\n\nTopic #26: concentr administr significantli combin increas studi anim compar observ signific measur decreas valu alon fluoresc bodi administ investig evalu reduc determin juic sensit induc receiv\n\nTopic #27: duoden ascit abdomin polyposi caus adenomat hospit effus examin diagnos gastrointestin hemorrhag multipl autopsi massiv famili admiss condit gener histori acut termin diagnosi comput jaundic\n\nTopic #28: esophag therapeut potenti studi strategi therapi provid improv includ diseas biolog advanc gastroesophag futur applic mani esophagu howev squamou variou mechan worldwid demonstr summar evid\n\nTopic #29: activ kinas enzym induc phosphoryl increas studi mechan respons metabol includ reduc enhanc demonstr potenti oxid deriv stimul epitheli involv indic mediat glutathion investig variou\n\nTopic #30: gastrectomi surgeri oper laparoscop surgic underw procedur complic radic hospit recurr lymphadenectomi abdomin feasibl intraop outcom advanc subtot earli preoper howev minim invas techniqu complet\n\nTopic #31: complic esophag anastomosi anastomot oper esophagectomi procedur esophagu techniqu nutrit leakag surgic underw oesophag studi compar proxim surgeri strictur jejun thorac empti cervic esophagogastr includ\n"
     ]
    }
   ],
   "source": [
    "print_top_words(lda, vectorizer, 25)"
   ]
  }
 ]
}